{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sandbox notebook for data and results exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "from pathlib import Path\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Cleaned data preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_path: Path = Path(\"../data/disaster/disaster_response.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(f\"sqlite:///{db_path}\")\n",
    "df = pd.read_sql(\"disaster_messages\", engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[\"message\"]\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_columns = [\n",
    "    col for col in df.columns if sorted(df[col].dropna().unique()) == [0, 1]\n",
    "]\n",
    "y = df[category_columns]\n",
    "y.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model test metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = pd.read_csv(\n",
    "    \"../data/models/performance_metrics.csv\", index_col=0, header=[0, 1]\n",
    ")\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Include in the README improvement suggestions (Challanges and further work, for example), such as the double loop for evaluation as well as improved multi-label stratification. Also mention one multi-label model strategy vs many one-label models.**\n",
    "There are many possibilities I can really try, but I don't have the time for it. I should just report what I tried, mention the challenges and potential improvements or different approaches that could be taken or stratesgies that could be followed. As long as I fulfill the project rubric, I should be able to pass the project and move on. This is the most important thing! Stop overdoing it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "disaster_response",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "25ed91f445a0a195739378b0771524fad1453778a7e34ac8cd87ac86085e7881"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
